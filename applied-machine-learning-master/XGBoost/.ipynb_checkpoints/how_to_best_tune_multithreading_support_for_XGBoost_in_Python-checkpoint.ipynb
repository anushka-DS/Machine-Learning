{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Best Tune Multithreading Support for XGBoost in Python\n",
    "The XGBoost library for gradient boosting uses is designed for efficient multi-core parallel processing.\n",
    "\n",
    "This allows it to efficiently use all of the CPU cores in your system when training.\n",
    "\n",
    "In this post you will discover the parallel processing capabilities of the XGBoost in Python.\n",
    "\n",
    "After reading this post you will know:\n",
    "\n",
    "* How to confirm that XGBoost multi-threading support is working on your system.\n",
    "* How to evaluate the effect of increasing the number of threads on XGBoost.\n",
    "* How to get the most out of multithreaded XGBoost when using cross validation and grid search.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description: Otto Dataset\n",
    "In this tutorial we will use the [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge) dataset.\n",
    "\n",
    "This dataset is available from Kaggle (you will need to sign-up to Kaggle to be able to download this dataset). You can download the training dataset train.zip from the [Data page](https://www.kaggle.com/c/otto-group-product-classification-challenge/data) and place the unzipped trian.csv file into your working directory.\n",
    "\n",
    "This dataset describes the 93 obfuscated details of more than 61,000 products grouped into 10 product categories (e.g. fashion, electrons, etc.). Input attributes are counts of different events of some kind.\n",
    "\n",
    "The goal is to make predictions for new products as an array of probabilities for each of the 10 categories and models are evaluated using multiclass logarithmic loss (also called cross entropy).\n",
    "\n",
    "This competition completed in May 2015 and this dataset is a good challenge for XGBoost because of the nontrivial number of examples and the difficulty of the problem and the fact that little data preparation is required (other than encoding the string class variables as integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of the Number of Threads\n",
    "XGBoost is implemented in C++ to explicitly make use of the [OpenMP API](https://en.wikipedia.org/wiki/OpenMP) for parallel processing.\n",
    "\n",
    "The parallelism in gradient boosting can be implemented in the construction of individual trees, rather than in creating trees in parallel like random forest. This is because in boosting, trees are added to the model sequentially. The speed of XGBoost is both in adding parallelism in the construction of individual trees, and in the efficient preparation of the input data to aid in the speed up in the construction of trees.\n",
    "\n",
    "Depending on your platform, you may need to compile XGBoost specifically to support multithreading. See the [XGBoost installation instructions](https://github.com/dmlc/xgboost/blob/master/doc/build.md) for more details.\n",
    "\n",
    "The **XGBClassifier** and **XGBRegressor** wrapper classes for XGBoost for use in scikit-learn provide the **nthread** parameter to specify the number of threads that XGBoost can use during training.\n",
    "\n",
    "By default this parameter is set to -1 to make use of all of the cores in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(nthread=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, you should get multithreading support for your XGBoost installation without any extra work.\n",
    "\n",
    "Depending on your Python environment (e.g. Python 3) you may need to explicitly enable multithreading support for XGBoost. The [XGBoost library provides an example](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_parallel.py) if you need help.\n",
    "\n",
    "You can confirm that XGBoost multi-threading support is working by building a number of different XGBoost models, specifying the number of threads and timing how long it takes to build each model. The trend will both show you that multi-threading support is enabled and give you an indication of the effect it has when building models.\n",
    "\n",
    "For example, if your system has 4 cores, you can train 8 different models and time how long in seconds it takes to create each, then compare the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the effect of the number of threads\n",
    "results = []\n",
    "num_threads = [1, 2, 3, 4]\n",
    "for n in num_threads:\n",
    "\tstart = time.time()\n",
    "\tmodel = XGBClassifier(nthread=n)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\telapsed = time.time() - start\n",
    "\tprint(n, elapsed)\n",
    "\tresults.append(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this approach on the Otto dataset. The full example is provided below for completeness.\n",
    "\n",
    "You can change the **num_threads** array to meet the number of cores on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Otto, tune number of threads\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# evaluate the effect of the number of threads\n",
    "results = []\n",
    "num_threads = [1, 2, 3, 4]\n",
    "for n in num_threads:\n",
    "\tstart = time.time()\n",
    "\tmodel = XGBClassifier(nthread=n)\n",
    "\tmodel.fit(X, label_encoded_y)\n",
    "\telapsed = time.time() - start\n",
    "\tprint(n, elapsed)\n",
    "\tresults.append(elapsed)\n",
    "# plot results\n",
    "pyplot.plot(num_threads, results)\n",
    "pyplot.ylabel('Speed (seconds)')\n",
    "pyplot.xlabel('Number of Threads')\n",
    "pyplot.title('XGBoost Training Speed vs Number of Threads')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example summarizes the execution time in seconds for each configuration, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(1, 115.51652717590332)\n",
    "(2, 62.7727689743042)\n",
    "(3, 46.042901039123535)\n",
    "(4, 40.55334496498108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of these timings is provided below.\n",
    "\n",
    "![number-of-threads-single-model](XGBoost-Tune-Number-of-Threads-for-Single-Model.png)\n",
    "\n",
    "We can see a nice trend in the decrease in execution time as the number of threads is increased.\n",
    "\n",
    "If you do not see an improvement in running time for each new thread, you may want to investigate how to enable multithreading support in XGBoost as part of your install or at runtime.\n",
    "\n",
    "We can run the same code on a machine with a lot more cores. The large Amazon Web Services EC2 instance is reported to have 32 cores. We can adapt the above code to time how long it takes to train the model with 1 to 32 cores. The results are plotted below.\n",
    "\n",
    "![time-to-train-model](XGBoost-Time-to-Train-Model-on-1-to-32-Cores.png)\n",
    "\n",
    "It is interesting to note that we do not see much improvement beyond 16 threads (at about 7 seconds). I expect the reason for this is that the Amazon instance only provides 16 cores in hardware and the additional 16 cores are available by hyperthreading. The results suggest that if you have a machine with hyperthreading, you may want to set **num_threads** to equal the number of physical CPU cores in your machine.\n",
    "\n",
    "The low-level optimized implementation of XGBoost with OpenMP squeezes every last cycle out of a large machine like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism When Cross Validating XGBoost Models\n",
    "The k-fold cross validation support in scikit-learn also supports multithreading.\n",
    "\n",
    "For example, the **n_jobs** argument on the **cross_val_score()** function used to evaluate a model on a dataset using k-fold cross validation allows you to specify the number of parallel jobs to run.\n",
    "\n",
    "By default, this is set to 1, but can be set to -1 to use all of the CPU cores on your system, which is good practice. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring='log_loss', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This raises the question as to how cross validation should be configured:\n",
    "\n",
    "* Disable multi-threading support in XGBoost and allow cross validation to run on all cores.\n",
    "* Disable multi-threading support in cross validation and allow XGBoost to run on all cores.\n",
    "* Enable multi-threading support for both XGBoost and Cross validation.\n",
    "\n",
    "We can get an answer to this question by simply timing how long it takes to evaluate the model in each circumstance.\n",
    "\n",
    "In the example below we use 10 fold cross validation to evaluate the default XGBoost model on the Otto training dataset. Each of the above scenarios is evaluated and the time taken to evaluate the model is reported.\n",
    "\n",
    "The full code example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Otto, parallel cross validation\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# prepare cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "# Single Thread XGBoost, Parallel Thread CV\n",
    "start = time.time()\n",
    "model = XGBClassifier(nthread=1)\n",
    "results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "elapsed = time.time() - start\n",
    "print(\"Single Thread XGBoost, Parallel Thread CV: %f\" % (elapsed))\n",
    "# Parallel Thread XGBoost, Single Thread CV\n",
    "start = time.time()\n",
    "model = XGBClassifier(nthread=-1)\n",
    "results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring='neg_log_loss', n_jobs=1)\n",
    "elapsed = time.time() - start\n",
    "print(\"Parallel Thread XGBoost, Single Thread CV: %f\" % (elapsed))\n",
    "# Parallel Thread XGBoost and CV\n",
    "start = time.time()\n",
    "model = XGBClassifier(nthread=-1)\n",
    "results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "elapsed = time.time() - start\n",
    "print(\"Parallel Thread XGBoost and CV: %f\" % (elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Single Thread XGBoost, Parallel Thread CV: 359.854589\n",
    "Parallel Thread XGBoost, Single Thread CV: 330.498101\n",
    "Parallel Thread XGBoost and CV: 313.382301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a benefit from parallelizing XGBoost over the cross validation folds. This makes sense as 10 sequential fast tasks is better than (10 divided by num_cores) slow tasks.\n",
    "\n",
    "Interestingly we can see that the best result is achieved by enabling both multi-threading within XGBoost and in cross validation. This is surprising because it means that num_cores number of parallel XGBoost models are competing for the same num_cores in the construction of their models. Nevertheless, this achieves the fastest results and is the suggested usage of XGBoost for cross validation.\n",
    "\n",
    "Because grid search uses the same underlying approach to parallelism, we expect the same finding to hold for optimizing the hyperparameters for XGBoost.\n",
    "\n",
    "## Summary\n",
    "In this post you discovered the multi-threading capability of XGBoost.\n",
    "\n",
    "You learned:\n",
    "\n",
    "* How to check that the multi-threading support in XGBoost is enabled on your system.\n",
    "* How increasing the number of threads affects the performance of training XGBoost models.\n",
    "* How to best configure XGBoost and Cross Validation in Python for minimum running time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
