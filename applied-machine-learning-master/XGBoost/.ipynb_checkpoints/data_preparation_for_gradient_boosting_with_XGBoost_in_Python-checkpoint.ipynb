{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Gradient Boosting with XGBoost in Python\n",
    "XGBoost is a popular implementation of Gradient Boosting because of its speed and performance.\n",
    "\n",
    "Internally, XGBoost models represent all problems as a regression predictive modeling problem that only takes numerical values as input. If your data is in a different form, it must be prepared into the expected format.\n",
    "\n",
    "In this post, you will discover how to prepare your data for using with gradient boosting with the XGBoost library in Python.\n",
    "\n",
    "After reading this post you will know:\n",
    "\n",
    "* How to encode string output variables for classification.\n",
    "* How to automatically handle missing data with XGBoost.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode String Class Values\n",
    "The iris flowers classification problem is an example of a problem that has a string class value.\n",
    "\n",
    "This is a prediction problem where given measurements of iris flowers in centimeters, the task is to predict to which species a given flower belongs.\n",
    "\n",
    "Below is a sample of the raw dataset. You can learn more about this dataset and download the raw data in CSV format from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost cannot model this problem as-is as it requires that the output variables be numeric.\n",
    "\n",
    "We can easily convert the string values to integer values using the LabelEncoder. The three class values (Iris-setosa, Iris-versicolor, Iris-virginica) are mapped to the integer values (0, 1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the label encoder as a separate object so that we can transform both the training and later the test and validation datasets using the same encoding scheme.\n",
    "\n",
    "Below is a complete example demonstrating how to load the iris dataset. Notice that Pandas is used to load the data in order to handle the string class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Accuracy: 92.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhnamle/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# multiclass classification\n",
    "import pandas\n",
    "import xgboost\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load data\n",
    "data = pandas.read_csv('iris.csv', header=None)\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]\n",
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice how the XGBoost model is configured to automatically model the multiclass classification problem using the **multi:softprob** objective, a variation on the softmax loss function to model class probabilities. This suggests that internally, that the output class is converted into a one hot type encoding automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for Missing Data\n",
    "XGBoost can automatically learn how to best handle missing data.\n",
    "\n",
    "In fact, XGBoost was designed to work with sparse data, like the one hot encoded data from the previous section, and missing data is handled the same way that sparse or zero values are handled, by minimizing the loss function.\n",
    "\n",
    "For more information on the technical details for how missing values are handled in XGBoost, see Section 3.4 “*Sparsity-aware Split Finding*” in the paper [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754).\n",
    "\n",
    "The Horse Colic dataset is a good example to demonstrate this capability as it contains a large percentage of missing data, approximately 30%.\n",
    "\n",
    "You can learn more about the Horse Colic dataset and download the raw data file from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/Horse+Colic).\n",
    "\n",
    "The values are separated by whitespace and we can easily load it using the Pandas function read_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"horse-colic.csv\", delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, we can see that the missing data is marked with a question mark character (‘?’). We can change these missing values to the sparse value expected by XGBoost which is the value zero (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set missing values to 0\n",
    "X[X == '?'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the missing data was marked as strings, those columns with missing data were all loaded as string data types. We can now convert the entire set of input data to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to numeric\n",
    "X = X.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is a binary classification problem although the class values are marked with the integers 1 and 2. We model binary classification problems in XGBoost as logistic 0 and 1 values. We can easily convert the Y dataset to 0 and 1 integers using the LabelEncoder, as we did in the iris flowers example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode Y class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Accuracy: 83.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhnamle/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# binary classification, missing data\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load data\n",
    "dataframe = read_csv(\"horse-colic.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:27]\n",
    "Y = dataset[:,27]\n",
    "# set missing values to 0\n",
    "X[X == '?'] = 0\n",
    "# convert to numeric\n",
    "X = X.astype('float32')\n",
    "# encode Y class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also impute the missing data with a specific value.\n",
    "\n",
    "It is common to use a mean or a median for the column. We can easily impute the missing data using the scikit-learn Imputer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values as the mean\n",
    "imputer = Imputer()\n",
    "imputed_x = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the full example with missing data imputed with the mean value from each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Accuracy: 79.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhnamle/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# binary classification, missing data, impute with mean\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "# load data\n",
    "dataframe = read_csv(\"horse-colic.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:27]\n",
    "Y = dataset[:,27]\n",
    "# set missing values to 0\n",
    "X[X == '?'] = numpy.nan\n",
    "# convert to numeric\n",
    "X = X.astype('float32')\n",
    "# impute missing values as the mean\n",
    "imputer = Imputer()\n",
    "imputed_x = imputer.fit_transform(X)\n",
    "# encode Y class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_x, label_encoded_y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example we see results equivalent to the fixing the value to one (1). This suggests that at least in this case we are better off marking the missing values with a distinct value of zero (0) rather than a valid value (1) or an imputed value.\n",
    "\n",
    "It is a good lesson to try both approaches (automatic handling and imputing) on your data when you have missing values.\n",
    "\n",
    "## Summary\n",
    "In this post you discovered how you can prepare your machine learning data for gradient boosting with XGBoost in Python.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* How to prepare string class values for binary classification using label encoding.\n",
    "* How XGBoost automatically handles missing data and how you can mark and impute missing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
