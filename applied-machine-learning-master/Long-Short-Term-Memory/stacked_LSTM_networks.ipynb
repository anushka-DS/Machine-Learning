{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Long Short-Term Memory Networks\n",
    "The original LSTM model is comprised of a single hidden LSTM layer followed by a standard feedforward output layer.\n",
    "\n",
    "The Stacked LSTM is an extension to this model that has multiple hidden LSTM layers where each layer contains multiple memory cells.\n",
    "\n",
    "In this post, you will discover the Stacked LSTM model architecture.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "* The benefit of deep neural network architectures.\n",
    "* The Stacked LSTM recurrent neural network architecture.\n",
    "* How to implement stacked LSTMs in Python with Keras.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This post is divided into 3 parts, they are:\n",
    "\n",
    "1. Why Increase Depth?\n",
    "2. Stacked LSTM Architecture\n",
    "3. Implement Stacked LSTMs in Keras\n",
    "\n",
    "## Why Increase Depth?\n",
    "Stacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique.\n",
    "\n",
    "It is the depth of neural networks that is generally attributed to the success of the approach on a wide range of challenging prediction problems.\n",
    "\n",
    "Additional hidden layers can be added to a Multilayer Perceptron neural network to make it deeper. The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction. For example, from lines to shapes to objects.\n",
    "\n",
    "A sufficiently large single hidden layer Multilayer Perceptron can be used to approximate most functions. Increasing the depth of the network provides an alternate solution that requires fewer neurons and trains faster. Ultimately, adding depth it is a type of representational optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM Architecture\n",
    "The same benefits can be harnessed with LSTMs.\n",
    "\n",
    "Given that LSTMs operate on sequence data, it means that the addition of layers adds levels of abstraction of input observations over time. In effect, chunking observations over time or representing the problem at different time scales.\n",
    "\n",
    "Stacked LSTMs or Deep LSTMs were introduced by Graves, et al. in their application of LSTMs to speech recognition, beating a benchmark on a challenging standard problem.\n",
    "\n",
    "In the same work, they found that the depth of the network was more important than the number of memory cells in a given layer to model skill.\n",
    "\n",
    "Stacked LSTMs are now a stable technique for challenging sequence prediction problems. A Stacked LSTM architecture can be defined as an LSTM model comprised of multiple LSTM layers. An LSTM layer above provides a sequence output rather than a single value output to the LSTM layer below. Specifically, one output per input time step, rather than one output time step for all input time steps.\n",
    "\n",
    "![architecture-stacked-LSTM](architecture_stacked_lstm.png)\n",
    "\n",
    "## Implement Stacked LSTMs in Keras\n",
    "We can easily create Stacked LSTM models in Keras Python deep learning library\n",
    "\n",
    "Each LSTMs memory cell requires a 3D input. When an LSTM processes one input sequence of time steps, each memory cell will output a single value for the whole sequence as a 2D array.\n",
    "\n",
    "We can demonstrate this below with a model that has a single hidden LSTM layer that is also the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08865715]]\n"
     ]
    }
   ],
   "source": [
    "# Example of one output for whole sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, input_shape=(3,1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# input time steps\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequence has 3 values. Running the example outputs a single value for the input sequence as a 2D array.\n",
    "\n",
    "To stack LSTM layers, we need to change the configuration of the prior LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "\n",
    "We can do this by setting the return_sequences argument on the layer to True (defaults to False). This will return one output for each input time step and provide a 3D array.\n",
    "\n",
    "Below is the same example as above with return_sequences=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00195488]\n",
      "  [0.00510129]\n",
      "  [0.00875532]]]\n"
     ]
    }
   ],
   "source": [
    "# Example of one output for each input time step\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, return_sequences=True, input_shape=(3,1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# input time steps\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example outputs a single value for each time step in the input sequence.\n",
    "\n",
    "Below is an example of defining a two hidden layer Stacked LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(..., return_sequences=True, input_shape=(...)))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to add hidden LSTM layers as long as the prior LSTM layer provides a 3D output as input for the subsequent layer; for example, below is a Stacked LSTM with 4 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(..., return_sequences=True, input_shape=(...)))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, you discovered the Stacked Long Short-Term Memory network architecture.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* The benefit of deep neural network architectures.\n",
    "* The Stacked LSTM recurrent neural network architecture.\n",
    "* How to implement stacked LSTMs in Python with Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
