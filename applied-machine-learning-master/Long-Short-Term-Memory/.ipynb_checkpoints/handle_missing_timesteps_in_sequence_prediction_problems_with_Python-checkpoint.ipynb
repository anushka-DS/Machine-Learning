{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Handle Missing Timesteps in Sequence Prediction Problems with Python\n",
    "It is common to have missing observations from sequence data.\n",
    "\n",
    "Data may be corrupt or unavailable, but it is also possible that your data has variable length sequences by definition. Those sequences with fewer timesteps may be considered to have missing values.\n",
    "\n",
    "In this tutorial, you will discover how you can handle data with missing values for sequence prediction problems in Python with the Keras deep learning library.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "* How to remove rows that contain a missing timestep.\n",
    "* How to mark missing timesteps and force the network to learn their meaning.\n",
    "* How to mask missing timesteps and exclude them from calculations in the model.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This section is divided into 3 parts; they are:\n",
    "\n",
    "1. Echo Sequence Prediction Problem\n",
    "2. Handling Missing Sequence Data\n",
    "3. Learning With Missing Sequence Values\n",
    "\n",
    "### Environment\n",
    "This tutorial assumes you have a Python SciPy environment installed. You can use either Python 2 or 3 with this example.\n",
    "\n",
    "This tutorial assumes you have Keras (v2.0.4+) installed with either the TensorFlow (v1.1.0+) or Theano (v0.9+) backend.\n",
    "\n",
    "This tutorial also assumes you have scikit-learn, Pandas, NumPy, and Matplotlib installed.\n",
    "\n",
    "## Echo Sequence Prediction Problem\n",
    "The echo problem is a contrived sequence prediction problem where the objective is to remember and predict an observation at a fixed prior timestep, called a lag observation.\n",
    "\n",
    "For example, the simplest case is to predict the observation from the previous timestep that is, echo it back. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Time 1: Input 45\n",
    "Time 2: Input 23, Output 45\n",
    "Time 3: Input 73, Output 23\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, what do we do about timestep 1?\n",
    "\n",
    "We can implement the echo sequence prediction problem in Python.\n",
    "\n",
    "This involves two steps: the generation of random sequences and the transformation of random sequences into a supervised learning problem.\n",
    "\n",
    "### Generate Random Sequence\n",
    "We can generate sequences of random values between 0 and 1 using the [random()](https://docs.python.org/3/library/random.html) function in the random module.\n",
    "\n",
    "We can put this in a function called generate_sequence() that will generate a sequence of random floating point values for the desired number of timesteps.\n",
    "\n",
    "This function is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame as Supervised Learning\n",
    "Sequences must be framed as a supervised learning problem when using neural networks.\n",
    "\n",
    "That means the sequence needs to be divided into input and output pairs.\n",
    "\n",
    "The problem can be framed as making a prediction based on a function of the current and previous timesteps.\n",
    "\n",
    "Or more formally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y(t) = f(X(t), X(t-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where y(t) is the desired output for the current timestep, f() is the function we are seeking to approximate with our neural network, and X(t) and X(t-1) are the observations for the current and previous timesteps.\n",
    "\n",
    "The output could be equal to the previous observation, for example, y(t) = X(t-1), but it could as easily be y(t) = X(t). The model that we train on this problem does not know the true formulation and must learn this relationship.\n",
    "\n",
    "This mimics real sequence prediction problems where we specify the model as a function of some fixed set of sequenced timesteps, but we don’t know the actual functional relationship from past observations to the desired output value.\n",
    "\n",
    "We can implement this framing of an echo problem as a supervised learning problem in python.\n",
    "\n",
    "The [Pandas shift()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html) function can be used to create a shifted version of the sequence that can be used to represent the observations at the prior timestep. This can be concatenated with the raw sequence to provide the X(t-1) and X(t) input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = DataFrame(sequence)\n",
    "df = concat([df.shift(1), df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the values from the Pandas DataFrame as the input sequence (X) and use the first column as the output sequence (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify input and output data\n",
    "X, y = values, values[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together, we can define a function that takes the number of timesteps as an argument and returns X,y data for sequence learning called generate_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Problem Demonstration\n",
    "We can tie the generate_sequence() and generate_data() code together into a worked example.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      nan 0.6137393] => nan\n",
      "[0.6137393  0.68540607] => 0.6137392972071636\n",
      "[0.68540607 0.37637228] => 0.6854060732497181\n",
      "[0.37637228 0.0901033 ] => 0.37637228124488553\n",
      "[0.0901033  0.40802453] => 0.0901033000189041\n",
      "[0.40802453 0.60029364] => 0.4080245288521507\n",
      "[0.60029364 0.34499509] => 0.600293644843831\n",
      "[0.34499509 0.44402892] => 0.34499509463998534\n",
      "[0.44402892 0.87593653] => 0.44402891960269364\n",
      "[0.87593653 0.57281337] => 0.8759365282435257\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    " \n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    " \n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y\n",
    " \n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(n_timesteps):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example generates a sequence, converts it to a supervised representation, and prints each X,y pair.\n",
    "\n",
    "We can see that we have NaN values on the first row.\n",
    "\n",
    "This is because we do not have a prior observation for the first value in the sequence. We have to fill that space with something.\n",
    "\n",
    "But we cannot fit a model with NaN inputs.\n",
    "\n",
    "## Handling Missing Sequence Data\n",
    "There are two main ways to handle missing sequence data.\n",
    "\n",
    "They are to remove rows with missing data and to fill the missing timesteps with another value.\n",
    "\n",
    "The best approach for handling missing sequence data will depend on your problem and your chosen network configuration. I would recommend exploring each method and see what works best.\n",
    "\n",
    "### Remove Missing Sequence Data\n",
    "In the case where we are echoing the observation in the previous timestep, the first row of data does not contain any useful information.\n",
    "\n",
    "That is, in the example above, given the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[        nan  0.18961404]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing meaningful that can be learned or predicted.\n",
    "\n",
    "The best case here is to delete this row.\n",
    "\n",
    "We can do this during the formulation of the sequence as a supervised learning problem by removing all rows that contain a NaN value. Specifically, the [dropna() function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) can be called prior to splitting the data into X and y components.\n",
    "\n",
    "The complete example is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35052025 0.68190401] => 0.350520247828497\n",
      "[0.68190401 0.15423214] => 0.6819040087491088\n",
      "[0.15423214 0.70453253] => 0.1542321378187086\n",
      "[0.70453253 0.32751965] => 0.7045325349998864\n",
      "[0.32751965 0.59273306] => 0.32751965121504834\n",
      "[0.59273306 0.48429259] => 0.5927330613071813\n",
      "[0.48429259 0.87226429] => 0.48429258539812303\n",
      "[0.87226429 0.55126978] => 0.8722642899949663\n",
      "[0.55126978 0.18220127] => 0.5512697765712204\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    " \n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    " \n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# remove rows with missing values\n",
    "\tdf.dropna(inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y\n",
    " \n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example results in 9 X,y pairs instead of 10, with the first row removed.\n",
    "\n",
    "### Replace Missing Sequence Data\n",
    "In the case when the echo problem is configured to echo the observation at the current timestep, then the first row will contain meaningful information.\n",
    "\n",
    "We can replace all NaN values with a specific value that does not appear naturally in the input, such as -1. To do this, we can use the [fillna() Pandas function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html).\n",
    "\n",
    "The complete example is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.          0.36248605] => 0.3624860490052806\n",
      "[0.36248605 0.38425938] => 0.38425938317697184\n",
      "[0.38425938 0.6159787 ] => 0.6159787016562683\n",
      "[0.6159787  0.95462465] => 0.9546246513863305\n",
      "[0.95462465 0.47350256] => 0.4735025625008039\n",
      "[0.47350256 0.49168172] => 0.49168172348867245\n",
      "[0.49168172 0.19306156] => 0.1930615572852208\n",
      "[0.19306156 0.03430595] => 0.03430595388105395\n",
      "[0.03430595 0.73127246] => 0.731272460984132\n",
      "[0.73127246 0.77012933] => 0.7701293327697278\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    " \n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    " \n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# replace missing values with -1\n",
    "\tdf.fillna(-1, inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 1]\n",
    "\treturn X, y\n",
    " \n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we can see that the NaN value in the first column of the first row was replaced with a -1 value.\n",
    "\n",
    "## Learning with Missing Sequence Values\n",
    "There are two main options when learning a sequence prediction problem with marked missing values.\n",
    "\n",
    "The problem can be modeled as-is and we can encourage the model to learn that a specific value means “missing.” Alternately, the special missing values can be masked and explicitly excluded from the prediction calculations.\n",
    "\n",
    "We will take a look at both cases for the contrived “echo the current observation” problem with two inputs.\n",
    "\n",
    "### Learning Missing Values\n",
    "We can develop an LSTM for the prediction problem.\n",
    "\n",
    "The input is defined by 2 timesteps with 1 feature. A small LSTM with 5 memory units in the first hidden layer is defined and a single output layer with a linear activation function.\n",
    "\n",
    "The network will be fit using the mean squared error loss function and the efficient ADAM optimization algorithm with default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(2, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the model learns a generalized solution to the problem, that is to always returns the input as output (y(t) == X(t)), we will generate a new random sequence every epoch. The network will be fit for 500 epochs and updates will be performed after each sample in each sequence (batch_size=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "for i in range(500):\n",
    "\tX, y = generate_data(n_timesteps)\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, another random sequence will be generated and the predictions from the model will be compared to the expected values. This will provide a concrete idea of the skill of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate model on new data\n",
    "X, y = generate_data(n_timesteps)\n",
    "yhat = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "\tprint('Expected', y[i,0], 'Predicted', yhat[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying all of this together, the complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 2s - loss: 0.1293\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2622\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3545\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2687\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1757\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2420\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2335\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1757\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1948\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1191\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0661\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1376\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0810\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1399\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0984\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1024\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1081\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1098\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1363\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0937\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0715\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0771\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0514\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0716\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0492\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0805\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0391\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0692\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0583\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0436\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0500\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0970\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0261\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0740\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0498\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0904\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0335\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0745\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0929\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0683\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0873\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0548\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0449\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0492\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0441\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1049\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0908\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0649\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0824\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0723\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0900\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0646\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0506\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0247\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0330\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0404\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0583\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0444\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0566\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0556\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0403\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0679\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1053\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0553\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0411\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0389\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0227\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0288\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0460\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0408\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0453\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0649\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0517\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0449\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0353\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0337\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0656\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0655\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0312\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0552\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0257\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0266\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0137\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0538\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0376\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0562\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0176\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0195\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0451\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0282\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0441\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0409\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0419\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0454\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0449\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0341\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0287\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0385\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0283\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0300\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0269\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0463\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0310\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0174\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0458\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0272\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0189\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0332\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0163\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0128\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0294\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0240\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0215\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0187\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0242\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0297\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0139\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0123\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0155\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0099\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0227\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0166\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0088\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0115\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0117\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0137\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0186\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0172\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0075\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0143\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0130\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0067\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0120\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0098\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0110\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0073\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0068\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0064\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0101\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0067\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0068\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.7953e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5663e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.5381e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.1952e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.6835e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.8160e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3515e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.2359e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.0281e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.1514e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5577e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9738e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.2566e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.3507e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1258e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0018e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.4435e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.5378e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4436e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.8983e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.4890e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7689e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.7450e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.6155e-04\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0034\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.9142e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.2425e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.6755e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.1827e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.2550e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5286e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.2762e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1602e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.5916e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4320e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.3890e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.8376e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.9046e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9811e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4954e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.1408e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.1901e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1960e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.4712e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.6818e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1106e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.1320e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.6710e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3664e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7908e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.6730e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5045e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1920e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.9614e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.1309e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.5001e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.9801e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.5173e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5436e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0765e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8320e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2670e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.4200e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0187e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0742e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.9368e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.0184e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2027e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5006e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4739e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.5731e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.4454e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.8887e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.8691e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7803e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4477e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0762e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.3138e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8049e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3531e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3962e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3584e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.2697e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1330e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.2834e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8755e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1292e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.1070e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.3212e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4430e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3352e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.4203e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7204e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.9963e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5987e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.6217e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2300e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2302e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5463e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4230e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0728e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5729e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5781e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2751e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.6471e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.3269e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4766e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.0415e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2484e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.2418e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7319e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7435e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7607e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6260e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9222e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3567e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.9511e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5221e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2764e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9078e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3179e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7681e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.0141e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7117e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.7290e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9501e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3398e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.4937e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6405e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.8744e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5751e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.5462e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6049e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1718e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.4078e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3979e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4194e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7093e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3288e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4554e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7655e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.3362e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1092e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8757e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1093e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7291e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7686e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2870e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7737e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2929e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.3861e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0041e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.6685e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.1401e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.7917e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.6828e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5183e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6028e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.1065e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8711e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0713e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7036e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.8478e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.0276e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8764e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6876e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3880e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8770e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3415e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1558e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5137e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3140e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7117e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6168e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1772e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8626e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7012e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5748e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7762e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5759e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.8021e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4667e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.5924e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.4343e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.1007e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0426e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3536e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0091e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.1611e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3564e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1770e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4848e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.3551e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2261e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8765e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6194e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7040e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.6893e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.8463e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6638e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.7481e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.1132e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7377e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.1610e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1638e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.0680e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.3367e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0106e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9704e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.8207e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.4612e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5328e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0701e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7595e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.4498e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2145e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0294e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.4102e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5086e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.0420e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9430e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4669e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.9511e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4949e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2832e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1867e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6955e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1630e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.9259e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6080e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2570e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.2050e-05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2897e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8831e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4364e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9717e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 6.0519e-05\n",
      "Expected 0.8685285196762417 Predicted 0.8194692\n",
      "Expected 0.1924922354886558 Predicted 0.20446332\n",
      "Expected 0.845564165498323 Predicted 0.8630018\n",
      "Expected 0.3894580702942654 Predicted 0.38488454\n",
      "Expected 0.37308305596938485 Predicted 0.3582573\n",
      "Expected 0.23948442298084527 Predicted 0.22121733\n",
      "Expected 0.8159167980950672 Predicted 0.8342422\n",
      "Expected 0.6328072753645948 Predicted 0.64054024\n",
      "Expected 0.049355718309846086 Predicted 0.06680457\n",
      "Expected 0.9487940714467635 Predicted 0.9472947\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    " \n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# replace missing values with -1\n",
    "\tdf.fillna(-1, inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 1]\n",
    "\t# reshape\n",
    "\tX = X.reshape(len(X), 2, 1)\n",
    "\ty = y.reshape(len(y), 1)\n",
    "\treturn X, y\n",
    " \n",
    "n_timesteps = 10\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape=(2, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit model\n",
    "for i in range(500):\n",
    "\tX, y = generate_data(n_timesteps)\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate model on new data\n",
    "X, y = generate_data(n_timesteps)\n",
    "yhat = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "\tprint('Expected', y[i,0], 'Predicted', yhat[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the loss each epoch and compares the expected vs. the predicted output at the end of a run for one sequence.\n",
    "\n",
    "Reviewing the final predictions, we can see that the network learned the problem and predicted “good enough” outputs, even in the presence of missing values.\n",
    "\n",
    "You could experiment further with this example and mark 50% of the t-1 observations for a given sequence as -1 and see how that affects the skill of the model over time.\n",
    "\n",
    "### Masking Missing Values\n",
    "The marked missing input values can be masked from all calculations in the network.\n",
    "\n",
    "We can do this by using a [Masking layer](https://keras.io/layers/core/#masking) as the first layer to the network.\n",
    "\n",
    "When defining the layer, we can specify which value in the input to mask. If all features for a timestep contain the masked value, then the whole timestep will be excluded from calculations.\n",
    "\n",
    "This provides a middle ground between excluding the row completely and forcing the network to learn the impact of marked missing values.\n",
    "\n",
    "Because the Masking layer is the first in the network, it must specify the expected shape of the input, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Masking(mask_value=-1, input_shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together and re-run the example. The complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 2s - loss: 0.4598\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6151\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3594\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4661\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3130\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1972\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4282\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4372\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2935\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3822\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1684\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2768\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3674\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2610\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1088\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1837\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0548\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2131\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3371\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1199\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1565\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1590\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1138\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1550\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0573\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0958\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1229\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1095\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0436\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0769\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0897\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0848\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0522\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0714\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0645\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1301\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0715\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0378\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0664\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0391\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0893\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1006\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0638\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0434\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0775\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0966\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0781\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0906\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0867\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1240\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0731\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0566\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0676\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0735\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1077\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0381\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0724\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0450\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1258\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0521\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0418\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0443\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0789\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0816\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0665\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0589\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0514\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1180\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0641\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0973\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0772\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0738\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0454\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0813\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0875\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0658\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0431\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0751\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0648\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0651\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0482\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0705\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0503\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0747\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0491\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0808\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0730\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0479\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0511\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0452\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0682\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0471\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0882\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0951\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0700\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0669\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0587\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0511\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0513\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0712\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0506\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0610\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0654\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0439\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0504\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0543\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0548\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0719\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0393\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0621\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0449\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0545\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0692\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0293\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0807\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0347\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0636\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0553\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0340\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0646\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0169\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0549\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0563\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0951\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0294\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0666\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0528\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0541\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0546\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0397\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0718\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0503\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0592\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0352\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1052\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0536\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0769\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0464\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0763\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0407\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0806\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0446\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0764\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0362\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0454\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0691\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0664\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0486\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0572\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0300\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0441\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0425\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0611\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0628\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0575\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0578\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0368\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0752\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0408\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0502\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0471\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0276\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0267\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0462\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0388\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0427\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0325\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0577\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0332\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0585\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0428\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0370\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0332\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0320\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0642\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0195\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0138\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0325\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0367\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0488\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0172\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0523\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0734\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0594\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0470\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0354\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0503\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0402\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0416\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0380\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0137\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0252\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0414\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0513\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0474\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0392\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0208\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0473\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0480\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0266\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0379\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0280\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0370\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0232\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0314\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0268\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0287\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0337\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0611\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0261\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0399\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0566\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0501\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0183\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0237\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0114\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0227\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0198\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0193\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0207\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0360\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0293\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0175\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0247\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0212\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0441\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0217\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0222\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0217\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0081\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0211\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0180\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0189\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0089\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0432\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0318\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0354\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0244\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0156\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0151\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0183\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0204\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0210\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0155\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0203\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0233\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0196\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0219\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0229\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0124\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0213\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0162\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0138\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0153\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0183\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0097\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0193\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0104\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0136\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0254\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0158\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0162\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0140\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0100\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0208\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0133\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0151\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0218\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0080\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0123\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0090\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0175\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0172\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0115\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0136\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0103\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0079\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0096\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0065\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0084\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0085\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.7187e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.7365e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9525e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.1608e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7914e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7781e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9735e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0110e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.4841e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.3256e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.6358e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.9635e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.7876e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.5567e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8690e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.1728e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.1654e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.9939e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.6743e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.5655e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.0109e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.6452e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.2415e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5688e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8785e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0076e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5923e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8029e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7424e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.3806e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7428e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.9176e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.3627e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0668e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7472e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.7050e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.5546e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.4866e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.7003e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4391e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.7237e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0680e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.2639e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.0599e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.6116e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.8700e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.8702e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.3640e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.6502e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.3947e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.7496e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.6799e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.7945e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.9785e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.9410e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.9252e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.7410e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.5715e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8071e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.7148e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0015\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.7219e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.3490e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.1786e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.9632e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.0332e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0014\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.5274e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.0375e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7.5787e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9.6680e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.5576e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.4480e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8.2627e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.6557e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4.4824e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5.9505e-04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6.1666e-04\n",
      "Expected 0.6934021459684614 Predicted 0.6519366\n",
      "Expected 0.09145052090961836 Predicted 0.10580151\n",
      "Expected 0.28752763885155364 Predicted 0.28294927\n",
      "Expected 0.6601537321388928 Predicted 0.65542126\n",
      "Expected 0.13920889514951384 Predicted 0.14367254\n",
      "Expected 0.8751777308771791 Predicted 0.87448454\n",
      "Expected 0.10268938077294332 Predicted 0.10023795\n",
      "Expected 0.11345668646043772 Predicted 0.14284211\n",
      "Expected 0.36837352504916787 Predicted 0.35574085\n",
      "Expected 0.4967746192490432 Predicted 0.48485774\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    " \n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    " \n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\t# replace missing values with -1\n",
    "\tdf.fillna(-1, inplace=True)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 1]\n",
    "\t# reshape\n",
    "\tX = X.reshape(len(X), 2, 1)\n",
    "\ty = y.reshape(len(y), 1)\n",
    "\treturn X, y\n",
    " \n",
    "n_timesteps = 10\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-1, input_shape=(2, 1)))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit model\n",
    "for i in range(500):\n",
    "\tX, y = generate_data(n_timesteps)\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate model on new data\n",
    "X, y = generate_data(n_timesteps)\n",
    "yhat = model.predict(X)\n",
    "for i in range(len(X)):\n",
    "\tprint('Expected', y[i,0], 'Predicted', yhat[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss is printed each epoch and the predictions are compared to expected values for a final sequence.\n",
    "\n",
    "Again, the predictions appear good enough to a few decimal places.\n",
    "\n",
    "### Which Method to Choose?\n",
    "These one-off experiments are not sufficient to evaluate what would work best on the simple echo sequence prediction problem.\n",
    "\n",
    "They do provide templates that you can use on your own problems.\n",
    "\n",
    "I would encourage you to explore the 3 different ways of handling missing values in your sequence prediction problems. They were:\n",
    "\n",
    "* Removing rows with missing values.\n",
    "* Mark and learn missing values.\n",
    "* Mask and learn without missing values.\n",
    "\n",
    "Try each approach on your sequence prediction problem and double down on what appears to work best.\n",
    "\n",
    "## Summary\n",
    "It is common to have missing values in sequence prediction problems if your sequences have variable lengths.\n",
    "\n",
    "In this tutorial, you discovered how to handle missing data in sequence prediction problems in Python with Keras.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* How to remove rows that contain a missing value.\n",
    "* How to mark missing values and force the model to learn their meaning.\n",
    "* How to mask missing values to exclude them from calculations in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
