{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Backpropagation Through Time\n",
    "Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks like LSTMs.\n",
    "\n",
    "To effectively frame sequence prediction problems for recurrent neural networks, you must have a strong conceptual understanding of what Backpropagation Through Time is doing and how configurable variations like Truncated Backpropagation Through Time will affect the skill, stability, and speed when training your network.In this post, you will get a gentle introduction to Backpropagation Through Time intended for the practitioner (no equations!).\n",
    "\n",
    "In this post, you will get a gentle introduction to Backpropagation Through Time intended for the practitioner (no equations!).\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "* What Backpropagation Through Time is and how it relates to the Backpropagation training algorithm used by Multilayer Perceptron networks.\n",
    "* The motivations that lead to the need for Truncated Backpropagation Through Time, the most widely used variant in deep learning for training LSTMs.\n",
    "* A notation for thinking about how to configure Truncated Backpropagation Through Time and the canonical configurations used in research and by deep learning libraries.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Training Algorithm\n",
    "Backpropagation refers to two things:\n",
    "\n",
    "* The mathematical method used to calculate derivatives and an application of the derivative chain rule.\n",
    "* The training algorithm for updating network weights to minimize error.\n",
    "\n",
    "It is this latter understanding of backpropagation that we are using here.\n",
    "\n",
    "The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs.\n",
    "\n",
    "It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made.\n",
    "\n",
    "The general algorithm is as follows:\n",
    "\n",
    "1. Present a training input pattern and propagate it through the network to get an output.\n",
    "2. Compare the predicted outputs to the expected outputs and calculate the error.\n",
    "3. Calculate the derivatives of the error with respect to the network weights.\n",
    "4. Adjust the weights to minimize the error.\n",
    "5. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A recurrent neural network is shown one input each timestep and predicts one output.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "1. Present a sequence of timesteps of input and output pairs to the network.\n",
    "2. Unroll the network then calculate and accumulate errors across each timestep.\n",
    "3. Roll-up the network and update weights.\n",
    "4. Repeat.\n",
    "\n",
    "BPTT can be computationally expensive as the number of timesteps increases.\n",
    "\n",
    "If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode (go to zero or overflow) and make slow learning and model skill noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Backpropagation Through Time\n",
    "Truncated Backpropagation Through Time, or TBPTT, is a modified version of the BPTT training algorithm for recurrent neural networks where the sequence is processed one timestep at a time and periodically (k1 timesteps) the BPTT update is performed back for a fixed number of timesteps (k2 timesteps).\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "1. Present a sequence of k1 timesteps of input and output pairs to the network.\n",
    "2. Unroll the network then calculate and accumulate errors across k2 timesteps.\n",
    "3. Roll-up the network and update weights.\n",
    "4. Repeat\n",
    "\n",
    "The TBPTT algorithm requires the consideration of two parameters:\n",
    "\n",
    "* **k1**: The number of forward-pass timesteps between updates. Generally, this influences how slow or fast training will be, given how often weight updates are performed.\n",
    "* **k2**: The number of timesteps to which to apply BPTT. Generally, it should be large enough to capture the temporal structure in the problem for the network to learn. Too large a value results in vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can borrow the notation from Williams and Peng and refer to different TBPTT configurations as TBPTT(k1, k2).\n",
    "\n",
    "Using this notation, we can define some standard or common approaches:\n",
    "\n",
    "Note, here n refers to the total number of timesteps in the sequence:\n",
    "\n",
    "* **TBPTT(n,n)**: Updates are performed at the end of the sequence across all timesteps in the sequence (e.g. classical BPTT).\n",
    "* **TBPTT(1,n)**: timesteps are processed one at a time followed by an update that covers all timesteps seen so far (e.g. classical TBPTT by Williams and Peng).\n",
    "* **TBPTT(k1,1)**: The network likely does not have enough temporal context to learn, relying heavily on internal state and inputs.\n",
    "* **TBPTT(k1,k2), where k1<k2<n**: Multiple updates are performed per sequence which can accelerate training.\n",
    "* **TBPTT(k1,k2), where k1=k2**: A common configuration where a fixed number of timesteps are used for both forward and backward-pass timesteps (e.g. 10s to 100s).\n",
    "\n",
    "We can see that all configurations are a variation on TBPTT(n,n) that essentially attempt to approximate the same effect with perhaps faster training and more stable results.\n",
    "\n",
    "Canonical TBPTT reported in papers may be considered TBPTT(k1,k2), where k1=k2=h and h<=n, and where the chosen parameter is small (tens to hundreds of timesteps).\n",
    "\n",
    "In libraries like TensorFlow and Keras, things look similar and h defines the vectorized fixed length of the timesteps of the prepared data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, you discovered the Backpropagation Through Time for training recurrent neural networks.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* What Backpropagation Through Time is and how it relates to the Backpropagation training algorithm used by Multilayer Perceptron networks.\n",
    "* The motivations that lead to the need for Truncated Backpropagation Through Time, the most widely used variant in deep learning for training LSTMs.\n",
    "* A notation for thinking about how to configure Truncated Backpropagation Through Time and the canonical configurations used in research and by deep learning libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
