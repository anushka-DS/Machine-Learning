{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gentle Introduction to Generative Long Short-Term Memory Networks\n",
    "The Long Short-Term Memory recurrent neural network was developed for sequence prediction.\n",
    "\n",
    "In addition to sequence prediction problems. LSTMs can also be used as a generative model\n",
    "\n",
    "In this post, you will discover how LSTMs can be used as generative models.\n",
    "\n",
    "After completing this post, you will know:\n",
    "\n",
    "* About generative models, with a focus on generative models for text called language modeling.\n",
    "* Examples of applications where LSTM Generative models have been used.\n",
    "* Examples of how to model text for generative models with LSTMs.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "LSTMs can be used as a generative model.\n",
    "\n",
    "Given a large corpus of sequence data, such as text documents, LSTM models can be designed to learn the general structural properties of the corpus, and when given a seed input, can generate new sequences that are representative of the original corpus.\n",
    "\n",
    "The problem of developing a model to generalize a corpus of text is called language modeling in the field of natural language processing. A language model may work at the word level and learn the probabilistic relationships between words in a document in order to accurately complete a sentence and generate entirely new sentences. At its most challenging, language models work at the character level, learning from sequences of characters, and generating new sequences one character at a time.\n",
    "\n",
    "Although more challenging, the added flexibility of a character-level model allows new words to be generated, punctuation added, and the generation of any other structures that may exist in the text data.\n",
    "\n",
    "Language modeling is by far the most studied application of Generative LSTMs, perhaps because of the use of standard datasets where model performance can be quantified and compared. This approach has been used to generate text on a suite of interesting language modeling problems, such as:\n",
    "\n",
    "* Generating Wikipedia articles (including markup).\n",
    "* Generating snippets from great authors like Shakespeare.\n",
    "* Generating technical manuscripts (including markup).\n",
    "* Generating computer source code.\n",
    "* Generating article headlines.\n",
    "\n",
    "The quality of the results vary; for example, the markup or source code may require manual intervention to render or compile. Nevertheless, the results are impressive.\n",
    "\n",
    "The approach has also been applied to different domains where a large corpus of existing sequence information is available and new sequences can be generated one step at a time, such as:\n",
    "\n",
    "* Handwriting generation.\n",
    "* Music generation.\n",
    "* Speech generation.\n",
    "\n",
    "![automatic-handwriting-recognition](Example-of-LSTMs-used-in-Automatic-Handwriting-Generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative LSTMs\n",
    "A Generative LSTM is not really architecture, it is more a change in perspective about what an LSTM predictive model learns and how the model is used.\n",
    "\n",
    "We could conceivably use any LSTM architecture as a generative model. In this case, we will use a simple Vanilla LSTM.\n",
    "\n",
    "![LSTM-Architecture-Generative-Models](Vanilla-LSTM-Architecture-for-Generative-Models.png)\n",
    "\n",
    "In the case of a character-level language model, the alphabet of all possible characters is fixed. A one hot encoding is used both for learning input sequences and predicting output sequences.\n",
    "\n",
    "A one-to-one model is used where one step is predicted for each input time step. This means that input sequences may require specialized handling in order to be vectorized or formatted for efficiently training a supervised model.\n",
    "\n",
    "For example, given the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset would need to be constructed such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'h' => 'e'\n",
    "'e' => 'l'\n",
    "'l' => 'l'\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be presented as-is as a dataset of one time step samples, which could be quite limiting to the network (e.g. no BPTT).\n",
    "\n",
    "Alternately, it could be vectorized to a fixed-length input sequence for a many-to-one time step model, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "['h', 'e', 'l'] => 'l'\n",
    "['e', 'l', 'l'] => 'o'\n",
    "['l', 'l', 'o'] => ' '\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, a fixed-length output sequence for a one-to-many time step model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'h' => ['e', 'l', 'l']\n",
    "'e' => ['l', 'l', 'o']\n",
    "'l' => ['l', 'o', ' ']\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or some variation on these approaches.\n",
    "\n",
    "Note that the same vectorized representation would be required when making predictions, meaning that predicted characters would need to be presented as input for subsequent samples. This could be quite clumsy in implementation.\n",
    "\n",
    "The internal state of the network may need careful management, perhaps reset at choice locations in the input sequence (e.g. end of paragraph, page, or chapter) rather than at the end of each input sequence.\n",
    "\n",
    "## Summary\n",
    "In this post, you discovered the use of LSTMs as generative models.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* About generative models, with a focus on generative models for text called language modeling.\n",
    "* Examples of applications where LSTM Generative models have been used.\n",
    "* Examples of how to model text for generative models with LSTMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
