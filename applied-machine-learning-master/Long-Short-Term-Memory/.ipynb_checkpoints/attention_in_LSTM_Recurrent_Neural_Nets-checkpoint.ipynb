{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention in Long Short-Term Memory Recurrent Neural Networks\n",
    "The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.\n",
    "\n",
    "A limitation of the architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences.\n",
    "\n",
    "In this post, you will discover the attention mechanism for recurrent neural networks that seeks to overcome this limitation.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "* The limitation of the encode-decoder architecture and the fixed-length internal representation.\n",
    "* The attention mechanism to overcome the limitation that allows the network to learn where to pay attention in the input sequence for each item in the output sequence.\n",
    "* 5 applications of the attention mechanism with recurrent neural networks in domains such as text translation, speech recognition, and more.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem With Long Sequences\n",
    "The encoder-decoder recurrent neural network is an architecture where one set of LSTMs learn to encode input sequences into a fixed-length internal representation, and second set of LSTMs read the internal representation and decode it into an output sequence.\n",
    "\n",
    "This architecture has shown state-of-the-art results on difficult sequence prediction problems like text translation and quickly became the dominant approach.\n",
    "\n",
    "The encoder-decoder architecture still achieves excellent results on a wide range of problems. Nevertheless, it suffers from the constraint that all input sequences are forced to be encoded to a fixed length internal vector.\n",
    "\n",
    "This is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems.\n",
    "\n",
    "## Attention within Sequences\n",
    "Attention is the idea of freeing the encoder-decoder architecture from the fixed-length internal representation.\n",
    "\n",
    "This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence.\n",
    "\n",
    "Put another way, each item in the output sequence is conditional on selective items in the input sequence.\n",
    "\n",
    "This increases the computational burden of the model, but results in a more targeted and better-performing model.\n",
    "\n",
    "In addition, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.\n",
    "\n",
    "## Problem with Large Images\n",
    "Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images.\n",
    "\n",
    "As a result, a series of glimpses can be taken of a large image to formulate an approximate impression of the image before making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Examples of Attention in Sequence Prediction\n",
    "This section provides some specific examples of how attention is used for sequence prediction with recurrent neural networks.\n",
    "\n",
    "### 1. Attention in Text Translation\n",
    "The motivating example mentioned above is text translation.\n",
    "\n",
    "Given an input sequence of a sentence in French, translate and output a sentence in English. Attention is used to pay attention to specific words in the input sequence for each word in the output sequence.\n",
    "\n",
    "![attention-text-translation](Attentional-Interpretation-of-French-to-English-Translation.png)\n",
    "\n",
    "### 2. Attention in Image Descriptions\n",
    "Different from the glimpse approach, the sequence-based attentional mechanism can be applied to computer vision problems to help get an idea of how to best use the convolutional neural network to pay attention to images when outputting a sequence, such as a caption.\n",
    "\n",
    "Given an input of an image, output an English description of the image. Attention is used to pay focus on different parts of the image for each word in the output sequence.\n",
    "\n",
    "![attention-image-description](Attentional-Interpretation-of-Output-Words-to-Specific-Regions-on-the-Input-Images.png)\n",
    "\n",
    "### 3. Attention in Entailment\n",
    "Given a premise scenario and a hypothesis about the scenario in English, output whether the premise contradicts, is not related, or entails the hypothesis.\n",
    "\n",
    "For example:\n",
    "\n",
    "* premise: “*A wedding party taking pictures*“\n",
    "* hypothesis: “*Someone got married*“\n",
    "\n",
    "Attention is used to relate each word in the hypothesis to words in the premise, and vise-versa.\n",
    "\n",
    "![attention-entailment](Attentional-Interpretation-of-Premise-Words-to-Hypothesis-Words.png)\n",
    "\n",
    "### 4. Attention in Speech Recognition\n",
    "Given an input sequence of English speech snippets, output a sequence of phonemes.\n",
    "\n",
    "Attention is used to relate each phoneme in the output sequence to specific frames of audio in the input sequence.\n",
    "\n",
    "![attention-speech-recognition](Attentional-Interpretation-of-Output-Phoneme-Location-to-Input-Frames-of-Audio.png)\n",
    "\n",
    "### 5. Attention in Text Summarization\n",
    "Given an input sequence of an English article, output a sequence of English words that summarize the input.\n",
    "\n",
    "Attention is used to relate each word in the output summary to specific words in the input document.\n",
    "\n",
    "![attention-text-summarization](Attentional-Interpretation-of-Words-in-the-Input-Document-to-the-Output-Summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.\n",
    "* That attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.\n",
    "* That the approach has been used across different types sequence prediction problems include text translation, speech recognition, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
